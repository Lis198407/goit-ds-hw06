{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#  –æ—Ç—Ä–∏–º–∞—î–º–æ –¥–∞–Ω—ñ –∑ —Ñ–∞–π–ª—É\n",
    "data = pd.read_csv('Housing.csv')\n",
    "X = data[['area', 'bathrooms', 'bedrooms']].values\n",
    "y = data['price'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è- –¥–µ–Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(X):\n",
    "    X_min = X.min(axis=0)\n",
    "    X_max = X.max(axis=0)\n",
    "    X_normalized = (X - X_min) / (X_max - X_min)\n",
    "    return X_normalized\n",
    "\n",
    "def de_normalize_data(X_origin,X_normilized):\n",
    "    X_min = X_origin.min(axis=0)\n",
    "    X_max = X_origin.max(axis=0)\n",
    "    X_de_normalized = X_min + X_normilized*(X_max - X_min)\n",
    "    return X_de_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –Ω–∞–ø–∏—à—ñ—Ç—å —Ñ—É–Ω–∫—Ü—ñ—é –≥—ñ–ø–æ—Ç–µ–∑–∏ –ª—ñ–Ω—ñ–π–Ω–æ—ó —Ä–µ–≥—Ä–µ—Å—ñ—ó —É –≤–µ–∫—Ç–æ—Ä–Ω–æ–º—É –≤–∏–≥–ª—è–¥—ñ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis(X,W):\n",
    "    return np.dot(X, W) # –ø–µ—Ä–µ–º–Ω–æ–∂–∏—Ç–∏ –≤–µ–∫—Ç–æ—Ä –≤–∞–≥—ñ–≤ –Ω–∞ –≤–µ–∫—Ç–æ—Ä –æ–∑–Ω–∞–∫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## —Å—Ç–≤–æ—Ä—ñ—Ç—å —Ñ—É–Ω–∫—Ü—ñ—é –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω–Ω—è —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç —É –≤–µ–∫—Ç–æ—Ä–Ω–æ–º—É –≤–∏–≥–ª—è–¥—ñ;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X, y, W):\n",
    "\n",
    "    m = len(y)  # –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ä—è–¥–∫—ñ–≤ –∑ –¥–∞–Ω–∏–º–∏\n",
    "    predictions = hypothesis(X, W)  # —Ä–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –≥—ñ–ø–æ—Ç–µ–∑—É\n",
    "    errors = predictions - y  # —Ä–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –≤—Ç—Ä–∞—Ç\n",
    "    loss = (1 / (2 * m)) * np.dot(errors.T, errors)  # —Å–µ—Ä–µ–¥–Ω—å–æ-–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞ —Ñ—É–Ω–∫—Ü—ã—è –≤—Ç—Ä–∞—Ç\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## —Ä–µ–∞–ª—ñ–∑—É–π—Ç–µ –æ–¥–∏–Ω –∫—Ä–æ–∫ –≥—Ä–∞–¥—ñ—î–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫—É;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w, alpha):\n",
    "    m = len(y)  # –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ä—è–¥–∫—ñ–≤ –¥–∞–Ω–∏—Ö\n",
    "    predictions = hypothesis(X, w)  # —Ä–∞—Ö—É—î–º–æ –≥—ñ–ø–æ—Ç–µ–∑—É\n",
    "    errors = predictions - y  # —Ä–∞—Ö—É—î–º–æ –ø–æ–º–∏–ª–∫—É, —è–∫ —Ä—ñ–∑–Ω–∏—Ü—é\n",
    "    gradient = (1 / m) * np.dot(X.T, errors)  # —Ä–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –≥—Ä–∞–¥—ñ—î–Ω—Ç –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º—É –≤–∏—Ä–∞–∑—ñ\n",
    "    w = w - alpha * gradient  # –æ–Ω–æ–≤–ª—é—î–º–æ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –∑–Ω–∞–π–¥—ñ—Ç—å –Ω–∞–π–∫—Ä–∞—â—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ ùë§‚Éó w –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç—É –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –Ω–∞–ø–∏—Å–∞–Ω—ñ –≤–∞–º–∏ —Ñ—É–Ω–∫—Ü—ñ—ó, –ø—Ä–æ–≥–Ω–æ–∑—É—é—á—É —Ü—ñ–Ω—É –Ω–∞ –±—É–¥–∏–Ω–æ–∫ –∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ –ø–ª–æ—â—ñ, –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –≤–∞–Ω–Ω–∏—Ö –∫—ñ–º–Ω–∞—Ç —Ç–∞ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —Å–ø–∞–ª–µ–Ω—å;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters (gradient solution): [0.04282675 0.47714285 0.36001168 0.17611438]\n",
      "y:          [13300000 12250000 12250000 12215000 11410000 10850000 10150000 10150000  9870000  9800000]\n",
      "y_gradient: [ 7036628 10392013  7591861  7066929  5650583  8046150  8862038 12155035  5908142  5997270]\n"
     ]
    }
   ],
   "source": [
    "X_normalized = normalize_data(X)\n",
    "X_normalized = np.c_[np.ones(X_normalized.shape[0]), X_normalized]\n",
    "y_normalized = normalize_data(y)\n",
    "\n",
    "w_gradient = np.random.rand(X_normalized.shape[1]) # –¥–ª—è –ø–µ—Ä—à–æ—ó —ñ—Ç–µ—Ä–∞—Ü—ñ—ó - –¥–æ–≤—ñ–ª—å–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –≤—ñ–¥ 0 –¥–æ 1\n",
    "\n",
    "# –∑–∞–¥–∞—î–º–æ –ø–æ—á–∞—Ç–∫–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏\n",
    "alpha = 0.1  # Learning rate\n",
    "num_iterations = 20000  # Number of iterations\n",
    "mse_loss = 1\n",
    "prev_mse_loss = 2\n",
    "stopping_treshold = 0.0000000000000001 # –≤–∏—Ö—ñ–¥ –∑ —Ü–∏–∫–ª—É –ø–æ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —ñ—Ç–µ—Ä–∞—Ü—ñ–π, –∞–±–æ –ø–æ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—é —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç –º–µ–Ω—à–µ –Ω—ñ–∂ –Ω–∞ ...\n",
    "i=1\n",
    "\n",
    "while i<=num_iterations and (prev_mse_loss-mse_loss) > stopping_treshold:\n",
    "    prev_mse_loss = mse_loss\n",
    "    w_gradient = gradient_descent(X_normalized, y_normalized, w_gradient, alpha)\n",
    "    mse_loss = compute_loss(X_normalized, y_normalized, w_gradient)\n",
    "    # print(f'step: {i}, mse_loss {mse_loss}, prev_mse_loss {prev_mse_loss}, w: {w}') #–ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø–æ–∫—Ä–æ–∫–æ–≤–æ\n",
    "    i +=1\n",
    "    \n",
    "y_gradient = hypothesis(X_normalized, w_gradient)\n",
    "y_gradient = de_normalize_data(y,y_gradient)\n",
    "print(\"Optimal parameters (gradient solution):\", w_gradient)\n",
    "print(f'y:          {y[0:10]}')\n",
    "print(f'y_gradient: {y_gradient[0:10].astype(int)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –∑–Ω–∞–π–¥—ñ—Ç—å —Ü—ñ –∂ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω–æ–≥–æ —Ä—ñ—à–µ–Ω–Ω—è;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters (analytical solution): [0.0428274  0.47714269 0.36001286 0.17611257]\n",
      "y:             [13300000 12250000 12250000 12215000 11410000 10850000 10150000 10150000  9870000  9800000]\n",
      " analytical_y: [ 7036627 10392020  7591864  7066928  5650577  8046157  8862041 12155033  5908136  5997273]\n"
     ]
    }
   ],
   "source": [
    "def analytical_solution(X, y):\n",
    "    w = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    return w\n",
    "\n",
    "X_normalized = normalize_data(X)\n",
    "X_normalized = np.c_[np.ones(X_normalized.shape[0]), X_normalized]\n",
    "\n",
    "y_normalized = normalize_data(y)\n",
    "\n",
    "w_analytical = analytical_solution(X_normalized, y_normalized)\n",
    "y_analytical = hypothesis(X_normalized,w_analytical)\n",
    "y_analytical = de_normalize_data(y,y_analytical)\n",
    "print(\"Optimal parameters (analytical solution):\", w_analytical)\n",
    "print(f'y:             {y[0:10]}')\n",
    "print(f' analytical_y: {y_analytical[0:10].astype(int)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Å–ø—Ä–æ–≥–Ω–æ–∑–æ–≤–∞–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å, –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–π—Ç–µ LinearRegression –∑ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏ scikit-learn —Ç–∞ –ø–æ—Ä—ñ–≤–Ω—è–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters (sklearn solution): 0.04282739976995403, [0.47714269 0.36001286 0.17611257]\n",
      "        y: [13300000 12250000 12250000 12215000 11410000 10850000 10150000 10150000  9870000  9800000]\n",
      "y_sklearn: [ 7036627 10392020  7591864  7066928  5650577  8046157  8862041 12155033  5908136  5997273]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True,\n",
    "   formatter={'float_kind':'{:0.2f}'.format})\n",
    "\n",
    "\n",
    "np.set_printoptions(edgeitems=3)\n",
    "np.core.arrayprint._line_width = 80\n",
    "\n",
    "model_sklearn = LinearRegression()\n",
    "\n",
    "X_normalized = normalize_data(X)\n",
    "y_normalized = normalize_data(y)\n",
    "\n",
    "model_sklearn.fit(X_normalized, y_normalized)\n",
    "\n",
    "coefficients_sklearn = model_sklearn.coef_\n",
    "intercept_sklearn = model_sklearn.intercept_\n",
    "\n",
    "y_sklearn = model_sklearn.predict(X_normalized)\n",
    "y_sklearn = de_normalize_data(y,y_sklearn)\n",
    "\n",
    "\n",
    "print(f\"Optimal parameters (sklearn solution): {intercept_sklearn}, {coefficients_sklearn}\")\n",
    "print(f'        y: {y[0:10]}')\n",
    "print(f'y_sklearn: {y_sklearn[0:10].astype(int)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters (gradient solution):   [0.04282807 0.4771423  0.36001408 0.17611083]\n",
      "Optimal parameters (analytical solution): [0.0428274  0.47714269 0.36001286 0.17611257]\n",
      "Optimal parameters (sklearn solution):    0.04282739976995403, [0.47714269 0.36001286 0.17611257]\n"
     ]
    }
   ],
   "source": [
    "# all decision in 1 output:\n",
    "print(\"Optimal parameters (gradient solution):  \", w_gradient)\n",
    "print(\"Optimal parameters (analytical solution):\", w_analytical)\n",
    "print(f\"Optimal parameters (sklearn solution):    {intercept_sklearn}, {coefficients_sklearn}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
